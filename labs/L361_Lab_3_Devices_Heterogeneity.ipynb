{"cells":[{"cell_type":"markdown","metadata":{"id":"2b1u66ZKmSCo"},"source":["# 0. Marking.\n","\n","***IMPORTANT***: Save a copy of this notebook into your Drive before you start.\n","- Please attempt all the questions marked for your group (Part II ✅ | Part III/MPhil ✅).\n","\n","Please submit a zip file, containing both parts, consiting of of:\n","1. A text file with a publicly visible link to your notebooks in Google Colab or GitHub.\n","2. A downloaded copy (ipynb) of your notebooks or your zipped cloned GitHub repo. You may treat these as a report: we will not be re-executing the code you used to produce the answers unless required.\n","\n","If you find yourself enjoying the material, feel free to attempt more! Provide your answers in a new cell below the question cell."]},{"cell_type":"markdown","metadata":{"id":"OUKMZp23mSCr"},"source":["# 1. Introduction\n"]},{"cell_type":"markdown","metadata":{"id":"R2DNy1LXmSCs"},"source":["Welcome to the third lab session in our FL course.\n","We now know how to “federate” a centralised ML model and have learned some tools to deal with data heterogeneity.\n","\n","During this session, we will try to put our hands on system heterogeneity in simulating FL. However, assuming you want to produce an FL system that will be deployed in the real world, taking into account data heterogeneity is often not enough.\n","\n","Client hardware may be correlated with the underlying data---e.g. a smartphone's camera impacts image characteristics---or with which data is used. For example, clients with less reliable internet connections from specific regions of the world are more likely to drop out, making their region underrepresented in training. Thus, the limits placed on computation time or the assumptions made in applying a synchronous or asynchronous algorithm affect model performance, fairness, or scalability.\n","\n","Developers may want to simulate their FL pipelines in a controlled environment in which natural system characteristics are modelled in order to be able to understand such trade-offs.\n","\n","What should we take into account, then?\n","Clients' availability, training time, communication bandwidth, and other factors sometimes impact natural FL systems unexpectedly.\n","Developers usually set up constraints for selecting clients to account for most of these factors, but in the worst case, these may result in not completing even a single round.\n","On the other hand, relaxing some constraints produces long round completion times, and eventually, the FL model cannot reach convergence in a reasonable interval.\n","\n","Before we begin modelling such concerns, we shall investigate data regarding real-world system characteristics from the two following papers:\n","\n","1. [Papaya: Practical, Private, and Scalable Federated Learning](https://arxiv.org/abs/2111.04877)\n","2. [Towards Federated Learning at Scale: System Design](https://arxiv.org/abs/1902.01046)\n"]},{"cell_type":"markdown","metadata":{"id":"yl5cv19OkMow"},"source":["# 2. Behavioural patterns of real-world FL systems\n"]},{"cell_type":"markdown","metadata":{"id":"w2YlPgJumSCs"},"source":["The following plot was extracted from the Papaya paper (1) mentioned above.\n","The takeaway is that in natural settings, devices are very heterogeneous, and the same model could take different amounts of time to train on various clients. Since the distribution is neither Gaussian nor uniform, accounting for such behaviour is non-trivial. As a result, developers often have to make imperfect choices when selecting between synchronous and asynchronous FL algorithms and their parameters.\n"]},{"cell_type":"markdown","metadata":{"id":"8AXtCGbgmSCt"},"source":["![client_execution_time](https://drive.google.com/uc?id=1-YwC63F2gFJrsM1KxoqJMgu-GAAgvjQx)\n"]},{"cell_type":"markdown","metadata":{"id":"zE3-F_r_kMow"},"source":["In the case of synchronous training, they may select clients based on hardware, set time limits for round completion, and incorporate partially trained client models that have not reached the necessary step/epoch count.\n","\n","Besides tackling the previously discussed data heterogeneity, FedProx was designed to better incorporate partially trained models from stragglers by limiting their impact upon the aggregation. The dual purpose of FedProx illustrates the interdependence of systems and data heterogeneity.\n","\n","In the case of asynchronous training, all updates **could** be considered; however, model staleness and a significant bias towards faster clients become major issues. For example, clients who return updates based on an older model may have had more data to train on or simply slower internet. If they have more data, they might provide more **statistical utility** than fast clients and thus improve final accuracy. Alternatively, they might have valuable data from underrepresented or remote regions if they have slower internet. If you are curious and want more insights about asynchronous training in FL, we point to the most prominent example: [FedBuf](https://arxiv.org/abs/2106.06639).\n","\n","In an extreme case of hardware heterogeneity, clients with high-end hardware may guide the model to the detriment of those with better data or those belonging to relevant subgroups. Due to such difficulties, asynchronous FL has been historically more difficult to implement and less preponderantly used than one may expect, given its potential benefits.\n"]},{"cell_type":"markdown","metadata":{"id":"Tv2XIa5qkMow"},"source":["**Question 1 (Part II ✅ | Part III/MPhil ✅):**\n","\n","(These are meant to be conceptual questions. You should provide written answers for these. **No more than 3 sentences each**. **No code** is needed)\n","\n","1. In the context of a complete synchronous FL training period composed of multiple rounds, when do you think it would be appropriate to oversample fast/slow clients? Should it be more appropriate for the early or late training phases? Think of which characteristics are essential for convergence in the early versus late rounds.\n","2. Can you think of a heuristic for modifying the time threshold across rounds that considers your previous answer?\n"]},{"cell_type":"markdown","metadata":{"id":"c4VcVSTNkMow"},"source":["**Question 2 (Part III/MPhil ✅):**\n","\n","(These are meant to be conceptual questions. You should provide written answers for these. **No more than 3 sentences each**. **No code** is needed)\n","\n","1. What parallels can be drawn between asynchronous SGD and asynchronous FL? Are there any methods which may help with both?\n","2. How do you think a system using a buffer to accumulate gradient updates received asynchronously, such as [FedBuf](https://arxiv.org/pdf/2106.06639.pdf), behaves?\n"]},{"cell_type":"markdown","metadata":{"id":"cDgqtaRHkMox"},"source":["## 2.1 Cyclical patterns\n"]},{"cell_type":"markdown","metadata":{"id":"dycOBwNSmSCt"},"source":["The plot below is taken from paper (2) mentioned above and encapsulates the reliability of client training. In the context of mobile devices, we can observe a cyclic daily trend for each category plotted while the delta between the categories is partly conserved. Thus, according to the number of clients we expect to finish training, we can adapt our training parameters, such as a completion-time threshold.\n","\n","If the federation included a more diverse set of devices operating in different domains or across a wider geographic area, the number of completing, aborted or dropped-out devices may not show the same synchronised patterns.\n"]},{"cell_type":"markdown","metadata":{"id":"gsqU3vK1mSCt"},"source":["![aborted_completed_dropped](https://drive.google.com/uc?id=1-NsWiw4-GM01OBYo2Qi_zSuOsVfjHFHc)\n"]},{"cell_type":"markdown","metadata":{"id":"e3vWOvDMmSCu"},"source":["These plots from paper (2) show a more clear cyclic trend in the participation rate, completion rate and network utilisation of clients depending on the day-night cycle. It is fair to say that these measurements were taken from devices in the same time zone and reflect common smartphone usage patterns related to humans' sleep and activity cycles.\n"]},{"cell_type":"markdown","metadata":{"id":"jCIR2EH5mSCu"},"source":["![connected_round_completion](https://drive.google.com/uc?id=1-NsGPsSfrjM5EUVk-9-OGRgL-IZbUEoh)\n"]},{"cell_type":"markdown","metadata":{"id":"Lh7CtpwtmSCv"},"source":["![network_traffic](https://drive.google.com/uc?id=1-ZQzDbmvXzNgcLw3xj8k3jhfN7ivlhdv)\n"]},{"cell_type":"markdown","metadata":{"id":"eSzyvZn0kMox"},"source":["**Question 3 (Part II ✅ | Part III/MPhil ✅):**\n","\n","(These are meant to be conceptual questions. You should provide written answers for these. **No more than 3 sentences each**. **No code** is needed)\n","\n","1. What would happen to device availability if we scaled the federated network globally?\n","2. How would the time of day interact with the client data and system characteristics in this new global federated network?\n"]},{"cell_type":"markdown","metadata":{"id":"kDH3cqcKkMox"},"source":["**Question 4 (Part III/MPhil ✅):**\n","\n","(These are meant to be conceptual questions. You should provide written answers for these. **No more than 3 sentences each**. **No code** is needed)\n","\n","Beyond shifts in the devices used at a specific time, on a long-enough timescale, FL also suffers from **non-cyclical** changes in the underlying hardware and data distributions which can lead to them getting desynced. For example, users may transfer their old data to a new device with more powerful sensors and thus create two “tasks'' instead of one. Furthermore, they can delete portions of the older data over time and shift the balance between the two tasks.\n","\n","1. For FL systems with more powerful and long-lasting clients, one potential method for handling such data and hardware shift is maintaining a persistent local model trained concurrently with the federated model---e.g. via [mutual knowledge distillation](https://arxiv.org/abs/1706.00384). This model may regularise the federated one and encodes valuable information about the data distribution accumulated at different points in time. Considering that such a model never leaves the client and thus does not have significant privacy concerns, what techniques would you apply to make it as **informative to the federated model as possible?**\n","\n","2. Assuming that the federated network has changed too much for an old federated model to perform well, how would you use the old model to bootstrap a new one?\n","\n","For both components of the question, you may draw inspiration from techniques used in [continual learning](https://arxiv.org/pdf/1909.08383.pdf), [mutual learning](https://arxiv.org/abs/1706.00384) or any other relevant field of ML.\n"]},{"cell_type":"markdown","metadata":{"id":"SnTGe6HTkMox"},"source":["# 3. Modelling system heterogeneity\n"]},{"cell_type":"markdown","metadata":{"id":"IYoFUSwGmSCv"},"source":["## Imports\n"]},{"cell_type":"markdown","metadata":{"id":"IhBh8fstmSCv"},"source":["The following cell will download the relevant python packages using `pip`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45960,"status":"ok","timestamp":1676716681758,"user":{"displayName":"Alexandru-Andrei Iacob","userId":"00751686620367198202"},"user_tz":0},"id":"NuHXppIqmSCv","outputId":"76b62fee-1187-4238-8d8d-ea0fc623c844"},"outputs":[],"source":["# The simulation component of flower uses RAY under the hood.\n","# `pip` could produce some errors. Nothing to worry about.\n","# The execution has been verified, it's working anyway.\n","! pip install --quiet --upgrade \"pip\"\n","! pip install --quiet git+https://github.com/Iacob-Alexandru-Andrei/flower.git@teaching torch torchvision matplotlib gdown tqdm ray==\"2.6.3\" seaborn torchsummary\n","# The following is just needed to show the folder tree\n","! apt-get install -qq tree"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AB_MjsSBmSCw"},"outputs":[],"source":["# Imports\n","import csv\n","import numbers\n","import os\n","import sys\n","import random\n","import pickle\n","from collections import OrderedDict, defaultdict\n","from collections.abc import Callable\n","from copy import deepcopy\n","from pathlib import Path\n","from typing import Any\n","from logging import INFO\n","import json\n","from datetime import timezone\n","from datetime import datetime\n","\n","\n","import flwr as fl\n","import ray\n","import gdown\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","import seaborn as sns\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from flwr.common import (\n","    log,\n","    Metrics,\n","    Config,\n","    GetPropertiesIns,\n","    GetPropertiesRes,\n","    MetricsAggregationFn,\n",")\n","from flwr.common.parameter import ndarrays_to_parameters\n","from flwr.common.typing import NDArrays, Parameters, Scalar\n","from flwr.server import ServerConfig, History\n","from flwr.server.server_returns_parameters import ReturnParametersServer as Server\n","from flwr.server.client_proxy import ClientProxy\n","from flwr.server.criterion import Criterion\n","from flwr.server.strategy import FedAvgM as FedAvg, Strategy\n","from flwr.client import Client\n","from PIL import Image\n","from PIL.Image import Image as ImageType\n","from torch.nn import Module\n","from torch.utils.data import DataLoader, Dataset, Subset\n","from torchvision import transforms\n","from tqdm import tqdm\n","from enum import IntEnum\n","from typing import cast\n","\n","\n","# Add new seeds here for easy autocomplete\n","class Seeds(IntEnum):\n","    DEFAULT = 1337\n","\n","\n","np.random.seed(Seeds.DEFAULT)\n","random.seed(Seeds.DEFAULT)\n","torch.manual_seed(Seeds.DEFAULT)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","\n","\n","PathType = Path | str | None"]},{"cell_type":"markdown","metadata":{"id":"QDJzls21mSCw"},"source":["In the following cell, we will download the relevant file we need for this session.\n","Feel free to look into this material if you want.\n","There is nothing new compared to Lab 2.\n"]},{"cell_type":"markdown","metadata":{"id":"d2LfZv9BmSCw"},"source":["### Dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["home_dir = content if (content := Path(\"/content\")).exists() else Path.cwd()\n","dataset_dir: Path = home_dir / \"femnist\"\n","data_dir: Path = dataset_dir / \"data\"\n","centralized_partition: Path = dataset_dir / \"client_data_mappings\" / \"centralized\"\n","centralized_mapping: Path = dataset_dir / \"client_data_mappings\" / \"centralized\" / \"0\"\n","federated_partition: Path = dataset_dir / \"client_data_mappings\" / \"fed_natural\"\n","# NEW\n","devices_info_dir: Path = home_dir / \"device_info\"\n","statistical_utility: Path = home_dir / \"statistical_utility.csv\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def convert(o: Any) -> int | float:\n","    \"\"\"Convert input object to Python numerical if numpy.\"\"\"\n","    # type: ignore[reportGeneralTypeIssues]\n","    if isinstance(o, np.int32 | np.int64):\n","        return int(o)\n","    # type: ignore[reportGeneralTypeIssues]\n","    if isinstance(o, np.float32 | np.float64):\n","        return float(o)\n","    raise TypeError\n","\n","\n","def save_history(hist: History, name: str) -> None:\n","    \"\"\"Save history from simulation to file.\"\"\"\n","    time = int(datetime.now(timezone.utc).timestamp())\n","    path = home_dir / \"histories\"\n","    path.mkdir(exist_ok=True)\n","    path = path / f\"hist_{time}_{name}.json\"\n","    with open(path, \"w\", encoding=\"utf-8\") as f:\n","        json.dump(hist.__dict__, f, ensure_ascii=False, indent=4, default=convert)\n","\n","\n","def start_seeded_simulation(\n","    client_fn: Callable[[str], Client],\n","    num_clients: int,\n","    config: ServerConfig,\n","    strategy: Strategy,\n","    name: str,\n","    seed: int = Seeds.DEFAULT,\n","    server: Server | None = None,\n","    iteration: int = 0,\n",") -> tuple[list[tuple[int, NDArrays]], History]:\n","    \"\"\"Wrap simulation to always seed client selection.\"\"\"\n","    np.random.seed(seed ^ iteration)\n","    torch.manual_seed(seed ^ iteration)\n","    random.seed(seed ^ iteration)\n","    parameter_list, hist = fl.simulation.start_simulation_no_ray(\n","        client_fn=client_fn,\n","        num_clients=num_clients,\n","        server=server,\n","        client_resources={},\n","        config=config,\n","        strategy=strategy,\n","    )\n","    save_history(hist, name)\n","    return parameter_list, hist"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":145828,"status":"ok","timestamp":1676716833352,"user":{"displayName":"Alexandru-Andrei Iacob","userId":"00751686620367198202"},"user_tz":0},"id":"Yuv_baiikMoy","outputId":"6905f7d3-b031-4ca8-e6e6-b64d88996ac2"},"outputs":[],"source":["#  Download compressed dataset\n","if not (home_dir / \"femnist.tar.gz\").exists():\n","    file_id = \"1-CI6-QoEmGiInV23-n_l6Yd8QGWerw8-\"\n","    gdown.download(\n","        f\"https://drive.google.com/uc?export=download&confirm=pbef&id={file_id}\",\n","        str(home_dir / \"femnist.tar.gz\"),\n","    )\n","\n","# Decompress dataset\n","if not dataset_dir.exists():\n","    !tar -xf {str(home_dir)}/femnist.tar.gz -C {str(home_dir)} 2> /dev/null\n","    log(INFO, f\"Dataset extracted in {dataset_dir}\")"]},{"cell_type":"markdown","metadata":{"id":"TJJU6WYlmSCx"},"source":["### Python files\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if not (home_dir / \"common\").exists():\n","    ! git clone \"https://github.com/camlsys/L361-Federated-Learning.git\" temp_repo\n","\n","    # Copy the folder to the current directory\n","    ! cp -r \"temp_repo/labs/common\" {home_dir}\n","\n","    # Delete the cloned repository\n","    ! rm -rf temp_repo\n","\n","    # Create the __init__.py file\n","    (home_dir / \"__init__.py\").open(mode=\"a+\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RFlvMIYykMoz"},"outputs":[],"source":["from common.client_utils import IntentionalDropoutError\n","from common.client import FlowerClient, get_flower_client_generator\n","from common.client_manager import CustomClientManager\n","from common.strategy import FedAvgTraces\n","from common.client_utils import (\n","    get_network_generator_cnn,\n","    get_model_parameters,\n","    aggregate_weighted_average,\n","    get_federated_evaluation_function,\n","    get_default_test_config,\n","    get_default_train_config,\n","    get_device,\n",")"]},{"cell_type":"markdown","metadata":{"id":"esYTgkRSmSCx"},"source":["## Extract system traces\n"]},{"cell_type":"markdown","metadata":{"id":"0hmoZgo6mSCx"},"source":["To model system heterogeneity, we will use a collection of actual device traces and capabilities extracted by another FL framework: [FedScale](https://github.com/SymbioticLab/FedScale). Unfortunately, these traces are not directly coupled to the FEMNIST dataset we have been using. As such, they cannot represent an intrinsic relation between data and system characteristics; we have to devise a mapping scheme between them. Generally, the pervasive lack of datasets synced to hardware characteristics in Federated Learning makes simulations unreliable as a source of guidance for production scenarios.\n","\n","A complete description of this data is available in [FedScale's](https://arxiv.org/abs/2105.11367) [paper](https://arxiv.org/abs/2105.11367) Sec. 3.2. Also, inside the folder you will download, there is a `README.md` file containing the minimal description of the files inside the folder.\n","\n","The first type of trace represents device communication and computation as floating point numbers. They can calculate a theoretical computation speed for each client for the given model, batch size and the number of batches.\n","\n","```\n","{\n","  'computation': FP32,\n","  'communication': FP32,\n","}\n","```\n","\n","The inherent assumption of the formula we shall use is that the relative ordering of devices does not change according to the task. Even if performance changes between tasks by orders of magnitude---and recent papers indicate that it can---as long as the ordering is conserved, we can adjust the thresholds of our experiments and obtain broadly consistent results. Explicitly, we assume that if one device performed inference faster than another in the original benchmark, it should do so for any model and data combination. However, this is not guaranteed to hold in a modern hardware landscape, and we would ideally need regularly updated system traces for every kind of data---e.g. image, text.\n","\n","A second type of trace contains data on client activity and is used to determine when it can be selected for training.\n","\n","```\n","{\n","  'duration': INT,\n","  'inactive': [INT],\n","  'finish_time': INT,\n","  'active': [INT],\n","  'model': STRING\n","}\n","```\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1725,"status":"ok","timestamp":1676716839326,"user":{"displayName":"Alexandru-Andrei Iacob","userId":"00751686620367198202"},"user_tz":0},"id":"n82bqNhCmSCx","outputId":"dbafa1fa-f107-438f-be71-be566b804dc1"},"outputs":[],"source":["# Download compressed dataset\n","if not (home_dir / \"device_info.tar.gz\").exists():\n","    id = \"1-9_C33KbSiuai9XJ6Cl4N-nmPltf5D6p\"\n","    gdown.download(\n","        f\"https://drive.google.com/uc?export=download&confirm=pbef&id={id}\",\n","        str(home_dir / \"device_info.tar.gz\"),\n","    )\n","# Decompress dataset\n","if not devices_info_dir.exists():\n","    !tar -xf {str(home_dir)}/device_info.tar.gz -C {str(home_dir)} 2> /dev/null\n","    log(INFO, \"Devices' info extracted in %s\", devices_info_dir)"]},{"cell_type":"markdown","metadata":{"id":"znAQLwX7mSCx"},"source":["We will implement three functions which will help us in the following discussion.\n","\n","1. The first `get_devices_info` can be used to put the info we just downloaded into `pandas` `DataFrame` structures.\n","2. The second `is_active` can compute whether a client is active, given its device traces and the virtual clock.\n","3. Finally, the third function `get_client_completion_time` is just computing the estimates for communication time and computation time of the client with the assumptions outlined above while incorporating a correction factor.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"an8QeXjxmSCx"},"outputs":[],"source":["def get_devices_info(\n","    root_dir: Path,\n",") -> tuple[pd.DataFrame, pd.DataFrame]:\n","    \"\"\"Retrieve two pandas data frames with traces and capabilities of clients' devices.\n","\n","    Args:\n","        root_dir (Path): path to the folder containing such traces.\n","\n","    Returns\n","    -------\n","        tuple[pd.DataFrame, pd.DataFrame]: couple of dataframes containing the\n","            requested info\n","    \"\"\"\n","    cbt_path = root_dir / \"client_behave_trace\"\n","    cdc_path = root_dir / \"client_device_capacity\"\n","    with open(cbt_path, \"rb\") as f:\n","        client_behave_trace = pd.DataFrame(pickle.load(f)).transpose()\n","    with open(cdc_path, \"rb\") as f:\n","        client_device_capacity = pd.DataFrame(pickle.load(f)).transpose()\n","    return client_behave_trace, client_device_capacity"]},{"cell_type":"markdown","metadata":{"id":"ZkrXB2AKkMoz"},"source":["Determining whether a client is active requires considering the cyclical behaviour described above. Specifically, our clock is defined by the `finish_time` integer inside the traces' data frame; the next cycle begins once the finish time is exceeded. We normalise all times by this `finish_time` value using the modulo operator. This normalised time allows us to maintain a virtual clock for client synchronisation in FL.\n","\n","Given the normalised time, each client is a sequence of active and inactive periods that wraps back at the end. The first inactive period is always chronologically after the first active period. However, not all the traces start at time 0. For consistency, we have considered the period between 0 and the beginning of the first active period to be **inactive**. This convention fits the idea that a client must join the federation before they can become active within it.\n","\n","A given client is considered active at the **start of a round** if its most recent active period before the current time is later than its most recent inactive period.\n","\n","> **IMPORTANT**: Activity at the start of the round **does not imply activity throughout the round**.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kras9RyHmSCx"},"outputs":[],"source":["def is_active(\n","    single_client_traces: dict[str, Any],\n","    current_clock_time: int,\n",") -> bool:\n","    \"\"\"Return a boolean describing whether the client is active or not.\n","\n","    It returns True when `single_client_traces` are not given.\n","    The current (virtual) clock time must pass as a parameter.\n","\n","    Args:\n","        single_client_traces (dict[str, Any]): dict describing client device traces.\n","        current_clock_time (int): parameter that describes current (virtual) clock time.\n","\n","    Returns\n","    -------\n","        bool: True is the client is active, False elsewhere.\n","    \"\"\"\n","    # If no traces are given, return True\n","    if single_client_traces is None:\n","        return True\n","    # Get the normalized time when the `current_clock_time` is\n","    # greater than `single_client_traces['finish_time']`, nothing\n","    # happens if `current_clock_time` < `single_client_traces['finish_time']`\n","    normalized_time = current_clock_time % single_client_traces[\"finish_time\"]\n","    # Get the highest single_client_traces['active'] occurrence\n","    # that is lower than `normalized_time`\n","    single_client_traces[\"active\"].sort()\n","    active_time = -1\n","    for t in single_client_traces[\"active\"]:\n","        active_time = t if t <= normalized_time else active_time\n","    # print(f\"Highest active time: {active_time}\")\n","    # Get the highest single_client_traces['inactive'] occurrence\n","    # that is lower than `normalized_time`\n","    single_client_traces[\"inactive\"].sort()\n","    inactive_time = -1\n","    for t in single_client_traces[\"inactive\"]:\n","        inactive_time = t if t <= normalized_time else inactive_time\n","    # print(f\"Highest inactive time: {inactive_time}\")\n","\n","    return active_time > inactive_time"]},{"cell_type":"markdown","metadata":{"id":"i3BPsUrtkMoz"},"source":["Estimating a given client's time to finish the workload is more straightforward, given our assumption of universal computation performance **across** tasks. However, one relevant detail is the need to double-count communication costs in the equation as the model needs to be transmitted both forwards and backwards. Please do not concern yourself with the normalisation of compute nor the scaling `augmentation_factor` unless you consider it necessary for a future experiment.\n","\n","One variable which will require tuning is `model_size_scale_factor`, as it allows us to model how the computation-communication trade-off changes when we increase or decrease the size of an actual ML model. For the rest of this lab the size of the model shall be assumed to be in MB.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"71ltPemfmSCy"},"outputs":[],"source":["def get_client_completion_time(\n","    single_client_device_capacity: dict[str, Any],\n","    batch_size: int,\n","    n_batches: int,\n","    model_size: float,\n","    augmentation_factor: float = 3.0,\n","    model_size_scale_factor: float = 1.0,\n",") -> dict[str, float]:\n","    \"\"\"Compute the computation and communication latency of the client.\n","\n","    These values are computed as follows:\n","    - Computation latency: `single_client_device_capacity['computation']` is the\n","        inference latency of models (ms/sample). We compute the computation latency as\n","        the inference latency times the number of samples processed. As reported in\n","        many papers, backward-pass takes around 2x the latency, so we multiply it by 3x.\n","    - Communication latency: `single_client_device_capacity['communication']` represents\n","        the bandwidth of the device (kB/s). We then compute the communication latency as\n","        the ratio between twice the size of the model and the bandwidth of the device.\n","\n","    Args:\n","        single_client_device_capacity (dict[str, Any]): dictionary containing info about\n","            device capabilities.\n","        batch_size (int): batch size used during local client training.\n","        n_batches (int): number of batches trained by the client.\n","        model_size (float): an estimate of the size of the model in MB.\n","        augmentation_factor (float, optional): multiplicative augmentation factor for\n","            the computation latency. Defaults to 3.0.\n","        model_size_scale_factor (float, optional): multiplicative augmentation factor\n","            for the communication latency. Defaults to 1.0.\n","\n","    Returns\n","    -------\n","        Dict[str, float]: dictionary containing estimates for time spent by the client\n","            in computation and communication.\n","    \"\"\"\n","    return {\n","        \"computation\": augmentation_factor\n","        * batch_size\n","        * n_batches\n","        * float(single_client_device_capacity[\"computation\"])\n","        / 1000.0,\n","        \"communication\": 2\n","        * model_size_scale_factor\n","        * model_size\n","        * 1000\n","        / float(single_client_device_capacity[\"communication\"]),\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cAaSFZYHkMoz"},"outputs":[],"source":["client_behave_trace, client_device_capacity = get_devices_info(devices_info_dir)"]},{"cell_type":"markdown","metadata":{"id":"CUCWOwKVmSCy"},"source":["### Analyse what's inside these traces\n"]},{"cell_type":"markdown","metadata":{"id":"qJ-mkQH9kMo0"},"source":["In the following few cells, we will analyse the traces regarding the distributions of the overall population.\n","The quantities we are primarily interested in are the `duration` and `finish_time` from `client_behave_trace`.\n"]},{"cell_type":"markdown","metadata":{"id":"rQAbYDUakMo0"},"source":["If we plot the duration of training for clients, the distribution should resemble the first plot we saw in this notebook. This `duration` value is the sum of the periods of time in which each device is active. Although, we will interpret it as the client execution time that can be computed as the sum of the outputs of `get_client_completion_time`. FedScale's authors used the `duration` value as the initial estimate for the client completion time. We will use it in this section as the client completion time for “standard” values of `batch_size` and `num_batches`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":278},"executionInfo":{"elapsed":1309,"status":"ok","timestamp":1676716868037,"user":{"displayName":"Alexandru-Andrei Iacob","userId":"00751686620367198202"},"user_tz":0},"id":"12ms9fFUmSCy","outputId":"93a42e37-02e9-45b0-d229-a30308b583d8"},"outputs":[],"source":["client_behave_trace.duration.hist(bins=\"auto\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"IpkTnZwVkMo0"},"source":["The subsequent distribution should show us how different the activity cycles recorded in the traces are. Due to the considerable variation in finish times, it was necessary to use the `finish_time` as a normalisation factor above to simulate devices operating in the same period.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":278},"executionInfo":{"elapsed":1069,"status":"ok","timestamp":1676716869101,"user":{"displayName":"Alexandru-Andrei Iacob","userId":"00751686620367198202"},"user_tz":0},"id":"ognZeZMYkMo0","outputId":"0f03c898-82ad-4c93-f74a-3d85d269674a"},"outputs":[],"source":["client_behave_trace.finish_time.hist(bins=\"auto\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"-3DbuYinwQXf"},"source":["One of the choices we had to make when modelling client activity concerned handling the period between 0 and when a client first becomes active; we can now inspect the motivating factors behind treating that period as inactive.\n","\n","Our choice was theoretically justifiable and practical given the start-times distribution we see below. The peak around 0 is natural, given that this was when the recording started. However, it may create difficulties during FL simulations as it would allow many clients to be available immediately for the first few rounds. Additionally, as all clients would be right at the beginning of their active period for the first round, it would create a scenario with minimal drop-out rates.\n","\n","> By treating everyone who starts later than 0 as inactive at the start, we lower the potential bias towards the early rounds that a simulation may have.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"elapsed":1751,"status":"ok","timestamp":1676716870850,"user":{"displayName":"Alexandru-Andrei Iacob","userId":"00751686620367198202"},"user_tz":0},"id":"ZetFsdEml6KZ","outputId":"1f87f22c-de62-42b1-c239-dff6c386eb38"},"outputs":[],"source":["# New column containing the start time of\n","# each client defined as the beginning\n","# of their first active period\n","client_behave_trace[\"first_active\"] = client_behave_trace[\"active\"].map(lambda x: x[0])\n","client_behave_trace.first_active.hist(bins=\"auto\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"AHYwXHoUkMo1"},"source":["**Question 5 (Part II ✅ | Part III/MPhil ✅):**\n","\n","(You need to provide the answer with **code** and **plots** for this question. A short written argumentation is recommended.)\n","\n","1. Randomly sample 100 behavioural traces from `client_behave_trace`. Sample without replacement. Plot the total number of active and inactive clients over time for 600000 seconds using steps of 10 seconds.\n","2. Sort `client_behave_trace` by `finish_time` in **ascending** order. Call it `ft_sorted_client_behave`. Repeat point (1) using `ft_sorted_client_behave` instead of `client_behave_trace`. Replace the random sampling with a sampling of the first 100 traces.\n","3. Sort `client_behave_trace` by `duration` in **descending** order. Call it `dur_sorted_client_behave`. Repeat point (1) using `dur_sorted_client_behave` instead of `client_behave_trace`. Replace the random sampling with a sampling of the first 100 traces.\n","4. Compare the three plots. What do you observe? What are the implications of performing FL with such populations?\n"]},{"cell_type":"markdown","metadata":{"id":"A9Fb96C19e2a"},"source":["**Question 6 (Part III/MPhil ✅):**\n","\n","(This is meant to be a conceptual question. You should provide a written answer to this. **No more than 3 sentences each**. **No code** is needed)\n","\n","1. In light of your consideration about question (5) above, what do you think could happen when reverting the ordering in point (2) and point (3)? Focus on the plots.\n","2. What are the implications of the FL training with such populations? Refer to populations with reverse ordering in point (2) and point (3).\n"]},{"cell_type":"markdown","metadata":{"id":"xKSr0evy97X4"},"source":["### Analyse what's inside device capabilities\n"]},{"cell_type":"markdown","metadata":{"id":"ssx7BRGG-LR0"},"source":["In the next few cells, we will analyse the content of `client_device_trace`.\n","Primarily, we will focus on the distribution of `computation` and `communication` on the clients and the afferent trade-off between the two, given a specific model size.\n"]},{"cell_type":"markdown","metadata":{"id":"GBVxtIFokMo0"},"source":["As we could have imagined, the devices' computational capabilities are highly heterogeneous. For example, despite the consistent peak at around 25 ms/sample, there is a relevant part of the population of devices with values 6 to 8 times higher. Suppose such heterogeneity in computation was related to factors influencing the data distribution. In that case, it could induce significant bias in the model training procedure, which would have to be considered by the developer.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1676716870851,"user":{"displayName":"Alexandru-Andrei Iacob","userId":"00751686620367198202"},"user_tz":0},"id":"O_02l1kakMo0","outputId":"79978ac3-7f7f-49d3-984d-617378947796"},"outputs":[],"source":["client_device_capacity.computation.hist(bins=\"auto\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"EOuygWirkMo0"},"source":["The communication capabilities are also very heterogeneous, as expected. Despite the distribution having a more apparent peak with a shorter tail, some devices have communication capabilities orders of magnitude higher than the peak of the distribution.\n","\n","Geographic variance in network speed is non-uniform and related to the data a given client may contain.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"elapsed":2059,"status":"ok","timestamp":1676716872906,"user":{"displayName":"Alexandru-Andrei Iacob","userId":"00751686620367198202"},"user_tz":0},"id":"L2jyeza-kMo0","outputId":"c3bc3b81-0d20-475b-a603-a593764965f7"},"outputs":[],"source":["client_device_capacity.communication.hist(bins=\"auto\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1676716872906,"user":{"displayName":"Alexandru-Andrei Iacob","userId":"00751686620367198202"},"user_tz":0},"id":"MKgeroBhp8bo","outputId":"0ebdc46b-206d-4171-fc3d-095eb1c6bb75"},"outputs":[],"source":["client_device_capacity.communication.mean()"]},{"cell_type":"markdown","metadata":{},"source":["As you may have previously noticed, the traces for the device compute capabilities and the traces for availability are uncorrelated. This is due to a lack of datasets in FL which synchronously cover both.\n","\n","In the following question you will be asked to numerically verify this lack of correlation. "]},{"cell_type":"markdown","metadata":{"id":"FQC-l65_08cW"},"source":["**Question 7 (Part II ✅):**\n","\n","(You need to provide the answer with **code** and **plots** for this question. A short written argumentation is recommended.)\n","\n","Looking carefully at the number in the y-axis, some of you may have noticed that the number of samples retrieved from `client_behave_trace` is not the same as that retrieved from `client_device_capacity`. Given this fact, we may wish to investigate if they are drawn from the same population of devices.\n","\n","1. Plot communication and computation mapped one-to-one using the entirety of `client_device_capacity`.\n","\n","- Do you notice any correlation between the two traces as given?\n","\n","2. Fix two different thresholds for computation, and plot the histogram of communication costs from the previous mapping using only traces whose computation falls below the given threshold.\n","\n","- Do these histograms look different from one another?\n","- Do they look different from the overall communication histogram?\n","\n","3. Fix two communication thresholds and plot the histograms of computation costs of traces with communication values falling below the thresholds.\n","\n","- Do these histograms look different from one another?\n","- Do they look different from the overall computation histogram?\n"]},{"cell_type":"markdown","metadata":{"id":"I6vjl2zFkMo0"},"source":["The mismatch between the two types of traces is because the authors of FedScale have combined two separate datasets to construct the files. They used [AI benchmark](https://arxiv.org/abs/1910.06663) to obtain the performance capabilities of mobile devices and [MobiPerf](https://www.measurementlab.net/tests/mobiperf/) to gather the availability traces.\n","\n","Similarly to the mismatch between traces and data we have previously discussed, experiments using performance and availability traces collected from separate devices may not generalise to the real world. However, since we do not yet have the large-scale datasets necessary to model FL systems accurately, we will have to accept this inconsistency.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1676716872907,"user":{"displayName":"Alexandru-Andrei Iacob","userId":"00751686620367198202"},"user_tz":0},"id":"IFVCdY_8kMo1","outputId":"bc3c3190-e429-422e-98ce-0290ac31bbd0"},"outputs":[],"source":["log(\n","    INFO,\n","    \"The number of samples in `client_behave_trace` is %s\",\n","    len(client_behave_trace),\n",")\n","log(\n","    INFO,\n","    \"The number of samples in `client_device_capacity` is %s\",\n","    len(client_device_capacity),\n",")"]},{"cell_type":"markdown","metadata":{"id":"RtWt7x2QkMo1"},"source":["Because the clients do not come from the same population, we need to devise a method for synchronising them.\n","The most straightforward procedure we can devise is to sample `len (client_behave_trace)` data points from `client_device_capacity` uniformly. Many other methods that can extract distributions with particular characteristics are possible, but we will do this for brevity.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1676716872907,"user":{"displayName":"Alexandru-Andrei Iacob","userId":"00751686620367198202"},"user_tz":0},"id":"4x5gU32_kMo1","outputId":"3fea272b-1f82-4ca1-ce03-cecac0d0ea99"},"outputs":[],"source":["client_device_capacity = client_device_capacity.sample(\n","    len(client_behave_trace), replace=False, random_state=Seeds.DEFAULT\n",")\n","log(\n","    INFO,\n","    \"The `client_device_capacity` obtained is %s\",\n","    client_device_capacity,\n",")"]},{"cell_type":"markdown","metadata":{"id":"1Sff3LPFkMo1"},"source":["We can plot the distributions from `client_device_capacity` to see if the sampling procedure has succeeded before we apply it in our experiments.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":515},"executionInfo":{"elapsed":1835,"status":"ok","timestamp":1676716874733,"user":{"displayName":"Alexandru-Andrei Iacob","userId":"00751686620367198202"},"user_tz":0},"id":"GDLgodTwp8bp","outputId":"04724a55-3a15-4364-afdc-219303058589"},"outputs":[],"source":["client_device_capacity.computation.hist(bins=\"auto\")\n","plt.show()\n","client_device_capacity.communication.hist(bins=\"auto\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"_oADe4R4sF96"},"source":["For later experiments, you will have to use a computation threshold to select clients. The following plot is likely the most revealing if you wish to understand better the impact a particular computation threshold will have on the number of clients that may be included in a given round. Pay particular attention to the sharp increase at the start of the plot.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"elapsed":329,"status":"ok","timestamp":1676716875053,"user":{"displayName":"Alexandru-Andrei Iacob","userId":"00751686620367198202"},"user_tz":0},"id":"Ono9HL-jsA2l","outputId":"0e153711-ebc0-4d2a-ab3a-183d47ac9ffe"},"outputs":[],"source":["client_device_capacity.computation.hist(bins=\"auto\", cumulative=True, density=True)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"hal8zjei_fUL"},"source":["**Question 8 (Part II ✅):**\n","\n","(You need to provide the answer with **code** and **plots** for this question. A short written argumentation is recommended.)\n","\n","We have just extracted and analysed the device capabilities of our population and obtained some idea about how communication and computation capabilities are distributed. However, the exact trade-off depends on `model_size`, `batch_size` and `n_batches`. Since `batch_size*n_batches` is fixed to equal the local dataset size, `model_size` requires closer attention for our modelling of FL systems to be informative. Specifically, we want both communication and computation to have detectable impacts on your FL experiments and their interaction shifts as you scale a network.\n","\n","Our function `get_client_completion_time` can help us observe the trade-off between computation and communication without running costly FL experiments. For the following question, assume that `batch_size=32`, `n_batches=64` stay fixed.\n","\n","1. Compute the average client completion time for different values of `model_size`. The average is intended to be over the population described by `client_device_capacity`. Choose at least 100 different values of the `model_size` in the interval `[0,2000] MB`.\n","2. Plot your results from (1) as explained here: from (1) you will have the average communication latency (`a`), the average computation latency (`b`), and you can compute their sum (`c`). Then, plot those three curves (`a`, `b`, and `c`) in the same graph VS model size. The intersection point between `a` and `b` must be visible. What does it represent?\n","3. Extract the model size from the intersection point in (2).\n","\n","> **You are required** to use the intersection parameter throughout the rest of this work.\n"]},{"cell_type":"markdown","metadata":{"id":"41vY3KUBmSC0"},"source":["# 4. Implementing system traces in the simulation\n"]},{"cell_type":"markdown","metadata":{"id":"bZxm5U12mSC0"},"source":["To implement our system traces into the Flower simulator to model a real FL federation, we need to extend the abstraction of `FlowerRayClient`. The new version of the abstraction shall include the information we have extracted from the traces. Achieving this will require creating a new class `FlowerRayClientTraces`, which inherits from `FlowerRayClient`.\n","\n","The new class will have minimal new attributes and functions needed to model the actual system accurately. We will simulate clients dropping out of the federation by throwing exceptions and allowing Flower to accept failures.\n","\n","The `get_flower_client_generator` function will be modified to return a generator of `FlowerRayClientTraces` instead of `FlowerRayClient`. In addition, we will pass the traces that must be coupled to the clients as parameters.\n"]},{"cell_type":"markdown","metadata":{"id":"n7w3rIsemSC0"},"source":["## Implementation\n"]},{"cell_type":"markdown","metadata":{"id":"vaqmSf5Up8bp"},"source":["We will also use previously unrevealed Flower features, such as client properties and criteria, so pay attention to those details. In short, clients can return properties to a criterion after they have been instantiated. The criterion then decides if the client should be included in the next federated round based on those properties.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tAw468w6kMo2"},"outputs":[],"source":["class FlowerClientTraces(FlowerClient):\n","    \"\"\"Extend the FlowerClient class to include traces and device capabilities.\"\"\"\n","\n","    def __init__(\n","        self,\n","        cid: int,\n","        data_dir: Path,\n","        partition_dir: Path,\n","        model_generator: Callable[[], Module],\n","        single_client_device_capacity: dict[str, Any],\n","        single_client_traces: dict[str, Any],\n","        verbose: bool = False,\n","    ) -> None:\n","        \"\"\"Initialise the client.\n","\n","        A Client is given a unique id and the directory from which it can load its data.\n","        Device capabilities and traces are also passed to the client.\n","\n","        Args:\n","            data_dir (Path): path to the dataset folder.\n","            cid (int): Unique client id for a client used to map it to its data\n","                partition\n","            partition_dir (Path): The directory containing data for each client/client\n","                id\n","            model_generator (Callable[[], Module]): The model generator function\n","            single_client_device_capacity (dict[str, Any]): dictionary containing info\n","                about device capabilities.\n","            single_client_traces (dict[str, Any]): dictionary describing client device\n","                traces.\n","            verbose (bool): boolean describing whether the client should print or not.\n","        \"\"\"\n","        super().__init__(cid, partition_dir, model_generator, data_dir)\n","        self.device_capacity = single_client_device_capacity\n","        self.trace = single_client_traces\n","        self.verbose = verbose\n","        self.properties: dict[str, Scalar] = {\n","            \"tensor_type\": \"numpy.ndarray\",\n","            \"cid\": self.cid,\n","            \"device_capacity\": self.device_capacity,\n","            \"traces\": self.trace,\n","        }\n","\n","    def fit(\n","        self, parameters: NDArrays, config: dict[str, Scalar], **kwargs\n","    ) -> tuple[NDArrays, int, dict]:\n","        \"\"\"Receive and train a model on the local client data.\n","\n","        Also, the function checks if the client is active at the current time step.\n","\n","        Args:\n","            net (NDArrays): Pytorch model parameters\n","            config (Dict[str, Scalar]): Dictionary describing the training parameters\n","\n","        Returns\n","        -------\n","            tuple[NDArrays, int, dict]: Returns the updated model, the size of the\n","                local dataset and other metrics\n","        \"\"\"\n","        # We need to include model size to compute communications costs as part of our\n","        # systems-aware simulation. Importantly, since Flower now accepts failures this\n","        # will only cause the client to return a failure\n","        if \"model_size\" not in config:\n","            raise Exception(\"Model size not found in config\")\n","\n","        # We need to compute the number of samples in the training set\n","        # As such we set n_batches to the number of batches which the set contains\n","        completion_time = get_client_completion_time(\n","            single_client_device_capacity=self.device_capacity,\n","            batch_size=int(config[\"batch_size\"]),\n","            n_batches=int(\n","                int(config[\"epochs\"])\n","                * self.get_train_set_size()\n","                / int(config[\"batch_size\"])\n","            ),\n","            model_size=float(config[\"model_size\"]),\n","        )\n","        # Add up the communication and computation times\n","        total_time = completion_time[\"communication\"] + completion_time[\"computation\"]\n","\n","        # Store the result in the trace\n","        self.trace[\"duration\"] = total_time\n","\n","        if self.verbose:\n","            log(\n","                INFO,\n","                \"Client %s\\n--------\\n\\t\\t\"\n","                \"Current virtual clock time: %s\\n\\t\\t\"\n","                \"Duration: %s\\n\\t\\t\"\n","                \"Traces: %s\\n\\t\\t\"\n","                \"Predicted completion: %s\\n\\t\\t\"\n","                \"Active: %s\\n--------\\n\",\n","                self.cid,\n","                config[\"current_virtual_clock\"],\n","                total_time,\n","                self.trace,\n","                int(config[\"current_virtual_clock\"]) + int(total_time),\n","                is_active(\n","                    self.trace, int(config[\"current_virtual_clock\"]) + int(total_time)\n","                ),\n","            )\n","        if \"current_virtual_clock\" not in config:\n","            raise IntentionalDropoutError(\"Current virtual clock not found in config\")\n","        if not is_active(\n","            self.trace, int(config[\"current_virtual_clock\"]) + int(total_time)\n","        ):\n","            raise IntentionalDropoutError(\n","                f\"Client {self.cid} is not active at the current time step\"\n","            )\n","\n","        n_samples = len(self._create_data_loader(config, \"train\"))\n","\n","        # NOTE: We do not need to train for this experiments\n","        # params, n_samples, results = super().fit(parameters, config)\n","        results = {\n","            \"client_completion_time\": total_time,\n","            \"computation\": self.device_capacity[\"computation\"],\n","            \"communication\": self.device_capacity[\"communication\"],\n","            \"num_samples\": n_samples,\n","            \"cid\": self.cid,\n","        }\n","\n","        return parameters, n_samples, results\n","\n","    def evaluate(\n","        self, parameters: NDArrays, config: dict[str, Scalar], **kwargs\n","    ) -> tuple[float, int, dict]:\n","        \"\"\"Receive and test a model on the local client data.\n","\n","        Also, the function checks if the client is active at the current time step.\n","\n","        Args:\n","            net (NDArrays): Pytorch model parameters\n","            config (Dict[str, Scalar]): Dictionary describing the testing parameters\n","\n","        Returns\n","        -------\n","            tuple[float, int, dict]: Returns the loss accumulate during testing, the\n","                size of the local dataset and other metrics such as accuracy\n","        \"\"\"\n","        if \"model_size\" not in config:\n","            raise Exception(\"Model size not found in config\")\n","\n","        # Estimate time based on number of batches in dataset\n","        completion_time = get_client_completion_time(\n","            single_client_device_capacity=self.device_capacity,\n","            batch_size=int(config[\"batch_size\"]),\n","            n_batches=int(self.get_test_set_size() / int(config[\"batch_size\"])),\n","            model_size=float(config[\"model_size\"]),\n","        )\n","\n","        # Compute total time\n","        total_time = completion_time[\"communication\"] + completion_time[\"computation\"]\n","\n","        # Store result in trace\n","        self.trace[\"duration\"] = total_time\n","        if self.verbose:\n","            log(\n","                INFO,\n","                \"Client %s\\n--------\\n\\t\\t\"\n","                \"Current virtual clock time: %s\\n\\t\\t\"\n","                \"Duration: %s\\n\\t\\t\"\n","                \"Traces: %s\\n\\t\\t\"\n","                \"Predicted completion: %s\\n\\t\\t\"\n","                \"Active: %s\\n--------\\n\",\n","                self.cid,\n","                config[\"current_virtual_clock\"],\n","                total_time,\n","                self.trace,\n","                int(config[\"current_virtual_clock\"]) + int(total_time),\n","                is_active(\n","                    self.trace, int(config[\"current_virtual_clock\"]) + int(total_time)\n","                ),\n","            )\n","        if \"current_virtual_clock\" not in config:\n","            raise IntentionalDropoutError(\"Current virtual clock not found in config\")\n","        if not is_active(\n","            self.trace, int(config[\"current_virtual_clock\"]) + int(total_time)\n","        ):\n","            raise IntentionalDropoutError(\n","                f\"Client {self.cid} is not active at the current time step\"\n","            )\n","\n","        n_samples = len(self._create_data_loader(config, \"test\"))\n","\n","        # NOTE: We do not need to train for this experiments\n","        # params, n_samples, results = super().fit(parameters, config)\n","        results = {\n","            \"client_completion_time\": total_time,\n","            \"computation\": self.device_capacity[\"computation\"],\n","            \"communication\": self.device_capacity[\"communication\"],\n","            \"num_samples\": n_samples,\n","            \"cid\": self.cid,\n","        }\n","\n","        return 0.0, n_samples, results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ppY8wuyGmSC0"},"outputs":[],"source":["def get_flower_client_with_traces_generator(\n","    clients_device_capacity: list[dict[str, Any]],\n","    clients_traces: list[dict[str, Any]],\n","    model_generator: Callable[[], Module],\n","    data_dir: Path,\n","    partition_dir: Path,\n","    mapping_fn: Callable[[int], int] | None = None,\n",") -> Callable[[str], FlowerClient]:\n","    \"\"\"Wrap the client instance generator.\n","\n","    This provides the client generator with a model generator function.\n","    Also, the partition directory must be passed.\n","    A mapping function could be used for filtering/ordering clients.\n","\n","    Args:\n","        clients_device_capacity (list[dict[str, Any]]): list containing the clients\n","            device capabilities.\n","        clients_traces (list[dict[str, Any]]): list containing the clients traces.\n","        data_dir (Path): path to the datasßet folder.\n","        model_generator (Callable[[], Module]): model generator function.\n","        partition_dir (Path): directory containing the partition.\n","        mapping_fn (Callable[[int], int] | None): function mapping sorted/filtered\n","            ids to real cid.\n","\n","    Returns\n","    -------\n","        Callable[[str], FlowerClient]: client instance.\n","    \"\"\"\n","\n","    def client_fn(cid: str) -> FlowerClientTraces:\n","        \"\"\"Create a single client instance given the client id `cid`.\n","\n","        Args:\n","            cid (str): client id, Flower requires this to of type str.\n","\n","        Returns\n","        -------\n","            FlowerRayClientTraces: client instance.\n","        \"\"\"\n","        log(INFO, \"Getting client with id %s\", cid)\n","        client = FlowerClientTraces(\n","            # NOTE: passing the called `cid` here to allow for different mapping between\n","            # data and devices\n","            single_client_device_capacity=clients_device_capacity[int(cid)],\n","            single_client_traces=clients_traces[int(cid)],\n","            # NOTE: the mapping is only applied here, this is due to control the data\n","            # mapping\n","            cid=(mapping_fn(int(cid)) if mapping_fn is not None else int(cid)),\n","            data_dir=data_dir,\n","            partition_dir=partition_dir,\n","            model_generator=model_generator,\n","            # NOTE: you may want to comment out the following line or to set the\n","            # verbosity to False\n","            verbose=False,\n","        )\n","\n","        client.device = get_device()\n","        return client\n","\n","    return client_fn"]},{"cell_type":"markdown","metadata":{"id":"8dj2MmIZmSC0"},"source":["# 5. Setting up selection criteria\n"]},{"cell_type":"markdown","metadata":{"id":"sFh0IFK1mSC0"},"source":["Selection criteria in FL may be varied; examples of such criteria include:\n","\n","1. The client's hardware capabilities; if they cannot train a given model (i.e., they lack a necessary accelerator), they should not be selected.\n","2. Network and battery conditions; if the client is a phone which lacks access to power and free Wi-Fi, it should not be selected.\n","3. Statistical utility; if a client seems likely to have data which interests us---based on previous selection or other characteristics such as provenance region---it should be selected to the detriment of other clients.\n","\n","> This lab shall focus on the first type of criteria. However, we shall build correlations between statistical utility and computational ability throughout to test how their inter-connection **could** impact FL under various assumptions.\n"]},{"cell_type":"markdown","metadata":{"id":"U2j_E6y-mSC0"},"source":["## Implement selection criteria using the Criterion object\n"]},{"cell_type":"markdown","metadata":{"id":"0HD30tfcp8c4"},"source":["In the following cell, an example of a `Criterion` object is provided.\n","Note that this does not discriminate between clients beyond checking their availabilities, but it is still helpful to understand how it functions. For later questions, you may desire to impose a selection criterion in your client selection based on this template.\n","\n","Be aware that using one of these objects implies additional communication between the client and the server. However, we will assume in our modelling that this communication is near-immediate (constant relative to the client dataset size) and is not affecting the federation by any means.\n","\n","It is worth mentioning that the potential design space of criteria in Flower is limited by the requirement of only communicating once with the client and only being able to receive data from the client's properties. These constraints make it impossible for the server to communicate data to the client at the criterion level so that the client may make dynamic decisions regarding participation. Furthermore, without a persistent communication channel, the criterion may become false before the round begins, and the client has no means of informing the server.\n","\n","To overcome these limitations, it would be valuable to expand and enhance the current implementation that Flower has under the hood for a potential Part II project.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ku1QX69mSC0"},"outputs":[],"source":["class TemplateCriterion(Criterion):\n","    \"\"\"Implements the Template criterion.\"\"\"\n","\n","    def __init__(self, **kwargs: Any) -> None:\n","        \"\"\"Initialize the Template criterion.\n","\n","        Args:\n","            **kwargs (Any): additional arguments.\n","        \"\"\"\n","        super().__init__(**kwargs)\n","\n","        \"\"\"\n","            Put here the logic to set global parameters\n","            for selection, s.a. thresholds.\n","        \"\"\"\n","\n","    def select(self, client: ClientProxy) -> bool:\n","        \"\"\"Select the client that receives.\n","\n","        Before selecting it, the function gets the `properties` of the client.\n","\n","        Args:\n","            client (ClientProxy): client proxy to select.\n","\n","        Returns\n","        -------\n","            bool: True if the client is selected, False otherwise.\n","        \"\"\"\n","        request_properties: Config = {\n","            # Here goes the template of the client properties\n","            \"tensor_type\": \"str\",\n","            \"cid\": \"str\",\n","            \"device_capacity\": \"Dict[str, Any]\",\n","            \"traces\": \"Dict[str, Any]\",\n","        }\n","        ins: GetPropertiesIns = GetPropertiesIns(config=request_properties)\n","        # NOTE: This `value` contains the client properties\n","        value: GetPropertiesRes = client.get_properties(ins, timeout=None)\n","        # NOTE: This object receives the current virtual clock from the server\n","        # in the CustomClientManager\n","        log(\n","            INFO,\n","            \"TemplateCriterion.select: current clock is %s\",\n","            self.current_virtual_clock,\n","        )\n","\n","        \"\"\"\n","            Put here the logic to select the client.\n","        \"\"\"\n","\n","        return True"]},{"cell_type":"markdown","metadata":{"id":"aWkif5hJQK_5"},"source":["Here follows the implementation of `ActivityCriterion`, a `Criterion` object that selects clients based on their activity evaluated at the selection time.\n","\n","> Selecting clients using this does not guarantee they will not drop out during the local training phase.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N4b-jlflQK_5"},"outputs":[],"source":["class ActivityCriterion(Criterion):\n","    \"\"\"Implements the a criterion based on the Activity.\"\"\"\n","\n","    def __init__(self) -> None:\n","        \"\"\"Initialize the Template criterion.\"\"\"\n","        self.current_virtual_clock = 0\n","\n","    def select(self, client: ClientProxy) -> bool:\n","        \"\"\"Select the client that receives if it is active.\n","\n","        Before selecting it, the function gets the `properties` of the client.\n","\n","        Args:\n","            client (ClientProxy): client proxy to select.\n","\n","        Returns\n","        -------\n","            bool: True if the client is selected, False otherwise.\n","        \"\"\"\n","        request_properties: Config = {\n","            # Here goes the template of the client properties\n","            \"tensor_type\": \"str\",\n","            \"cid\": \"str\",\n","            \"device_capacity\": \"Dict[str, Any]\",\n","            \"traces\": \"Dict[str, Any]\",\n","        }\n","        ins: GetPropertiesIns = GetPropertiesIns(config=request_properties)\n","        # NOTE: This contains the client properties\n","        value: GetPropertiesRes = client.get_properties(ins, timeout=None)\n","        # NOTE: here we use `is_active` to check if the client is active at the current\n","        # time step\n","        return is_active(value.properties[\"traces\"], self.current_virtual_clock)"]},{"cell_type":"markdown","metadata":{"id":"w1zEhkkSnttl"},"source":["The following cell is meant to initialise the model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zG4I-uALkMo2"},"outputs":[],"source":["torch.manual_seed(Seeds.DEFAULT)\n","network_generator_cnn = get_network_generator_cnn()\n","seed_net_cnn = network_generator_cnn()\n","seed_model_cnn_params: NDArrays = get_model_parameters(seed_net_cnn)"]},{"cell_type":"markdown","metadata":{"id":"cflS3GvdqI57"},"source":["Given that you have determined the intersection point within the traces we use, it is reasonable to consider the actual size of our model in MB. For this, we shall use the `torchsummary` package.\n","\n","> **KEEP USING THE INTERSECTION MB VALUE DETERMINED ABOVE**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":200,"status":"ok","timestamp":1676716875249,"user":{"displayName":"Alexandru-Andrei Iacob","userId":"00751686620367198202"},"user_tz":0},"id":"94nLEl7PqSVZ","outputId":"c6443fbd-4572-43e0-b798-ab4b7bb54837"},"outputs":[],"source":["import torchsummary\n","\n","torchsummary.summary(\n","    seed_net_cnn.to(device if (device := get_device()) != \"mps\" else \"cpu\"),\n","    (1, 28, 28),\n","    device=device if (device := get_device()) != \"mps\" else \"cpu\",\n",")"]},{"cell_type":"markdown","metadata":{"id":"9ZA3-tl_qZMs"},"source":["As we can see, the network we have selected for demonstration purposes in this lab is tiny and should thus result in meagre communication costs relative to computation. However, the balance between computation and communication would likely not be so one-sided in a production system.\n","\n","Thus far, we have treated the two costs as equivalent in terms of their impact on the overall training time of the FL system. However, the two are not always symmetrical in terms of costs. For example, if the entity running the federation does not own nor pay for the resources of the underlying devices---as is the case in many scenarios involving smartphones---then computation on-edge incurs zero monetary costs. At the same time, models' storage and transmission scales with the size and number of networks. Furthermore, while we only account for model size in the communication costs, a larger model requires more operations per training step and more training steps overall to train.\n","\n","All of these factors guide FL to be biased towards using architectures with a lower footprint, reducing model size via quantisation, and clustering clients into small groups that have similar data---which a small network can quickly learn from.\n"]},{"cell_type":"markdown","metadata":{"id":"jqjfmMxloHgM"},"source":["Here the function to generate clients is provided. Note that if you want to use a different `client_device_capacity` or `client_behave_trace`, you need to change the parameters here and re-execute the cell.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nTJt_W4LkMo3"},"outputs":[],"source":["# NOTE: we are using here the `get_flower_client_with_traces_generator`\n","num_total_clients = 3229\n","sampled_cdc = client_device_capacity.sample(\n","    num_total_clients, replace=False, random_state=Seeds.DEFAULT\n",")\n","sampled_cbt = client_behave_trace.sample(\n","    num_total_clients, replace=False, random_state=Seeds.DEFAULT\n",")\n","\n","flower_client_with_traces_generator: Callable[\n","    [str], FlowerClient\n","] = get_flower_client_with_traces_generator(\n","    clients_device_capacity=sampled_cdc.to_dict(\"records\"),\n","    clients_traces=sampled_cbt.to_dict(\"records\"),\n","    model_generator=network_generator_cnn,\n","    data_dir=data_dir,\n","    partition_dir=federated_partition,\n",")"]},{"cell_type":"markdown","metadata":{"id":"ynfJhLwloyUk"},"source":["The following cell contains the vast majority of the hyperparameters for the FL training. Notice the addition of a model size and virtual clock in the configs and the `accept_failures` = True` parameter.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aVuQziUckMo3"},"outputs":[],"source":["train_config = get_default_train_config()\n","train_config.update(\n","    {\n","        \"model_size\": 1000,\n","        \"current_virtual_clock\": 0.0,\n","    }\n",")\n","test_config = get_default_test_config()\n","test_config.update(\n","    {\n","        \"model_size\": 1000,\n","        \"current_virtual_clock\": 0.0,\n","    }\n",")\n","\n","\n","def _on_fit_config_fn(server_round: int) -> dict[str, Scalar]:\n","    return train_config | {\"server_round\": server_round}\n","\n","\n","def _on_evaluate_config_fn(server_round: int) -> dict[str, Scalar]:\n","    return test_config | {\"server_round\": server_round}\n","\n","\n","num_clients_per_round: int = 3\n","num_evaluate_clients: int = 1\n","fraction_fit = sys.float_info.min\n","fraction_evaluate = sys.float_info.min\n","accept_failures = True\n","min_fit_clients = num_clients_per_round\n","min_available_clients = num_evaluate_clients\n","initial_parameters: Parameters = ndarrays_to_parameters(seed_model_cnn_params)\n","\n","strategy = FedAvgTraces(\n","    fraction_fit=fraction_fit,\n","    fraction_evaluate=fraction_evaluate,\n","    min_fit_clients=min_fit_clients,\n","    min_available_clients=min_available_clients,\n","    on_fit_config_fn=_on_fit_config_fn,\n","    on_evaluate_config_fn=_on_evaluate_config_fn,\n","    initial_parameters=initial_parameters,\n","    accept_failures=accept_failures,\n","    evaluate_fn=None,\n","    fit_metrics_aggregation_fn=aggregate_weighted_average,\n","    evaluate_metrics_aggregation_fn=aggregate_weighted_average,\n",")"]},{"cell_type":"markdown","metadata":{"id":"fJqYeUZwpIOw"},"source":["Since we have introduced a `Criterion` object, we need to use both a `Server` and a `ClientManager`. The `Server` orchestrates training over selected clients while the `ClientManager` is tasked with providing it with the lists of clients used for a given round. Consequently, they will wrap our `Criterion` with all the utilities to let the FL simulation use it.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5GknkM_TkMo3"},"outputs":[],"source":["criterion = ActivityCriterion()\n","custom_client_manager = CustomClientManager(criterion=criterion, seed=Seeds.DEFAULT)\n","server = Server(\n","    client_manager=custom_client_manager,\n","    strategy=strategy,\n",")"]},{"cell_type":"markdown","metadata":{"id":"6CLoTEdEpnwc"},"source":["We may finally launch the simulation with enhanced abilities to select clients based on traces and to have them drop out according to their activity and execution duration.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MAQilC1ukMo3"},"outputs":[],"source":["model_params, hist = start_seeded_simulation(\n","    client_fn=lambda cid: flower_client_with_traces_generator(cid).to_client(),\n","    num_clients=num_total_clients,\n","    config=ServerConfig(num_rounds=10),\n","    server=server,\n","    strategy=strategy,\n","    name=\"activity_criterion\",\n",")"]},{"cell_type":"markdown","metadata":{"id":"lSdZ_9uMp8br"},"source":["Let us investigate the new history object you will use to determine the behaviour of your execution. Notice the addition of a completion time for each client, as they are meant to execute in parallel. The highest completion time represents the duration of a round. Our new strategy and criterion this principle to maintain the `current_virtual_clock`; you may investigate the implementation further by looking into the python files.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zx4M2o7V0dbd"},"outputs":[],"source":["log(INFO, \"Simulation results: %s\", hist)"]},{"cell_type":"markdown","metadata":{"id":"ODX5HxZwkMo3"},"source":["**Question 9 (Part II ✅):**\n","\n","(You need to provide the answer with **code** and **plots** for this question. An extended written argumentation is recommended.)\n","\n","Now that we can run a whole synchronised system-aware simulation, it is time we consider how different system characteristics can impact client availability and in-turn the distribution of data the model trains on.\n","\n","1. Produce a random mapping between client IDs and traces, this mapping should be fixed and used across all rounds and experiments of this question.\n","2. Assume an FL setting whose total population is of 3229 clients and assume random sampling of 10 clients per round and 323 rounds.\n","3. Implement three different criteria to obtain the following three availability patterns respectively:\n","  - Pattern 1: Constant availability of 10 clients per round.\n","  - Pattern 2: Average availability of 8 clients per round.\n","  - Pattern 3: Average availability of 5 clients per round.\n","4. Support your implementation choices with both: (a) a priori discussion, (b) a discussion of the plots and results.\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["(c) 2024 Alexandru-Andrei Iacob, Lorenzo Sani"]}],"metadata":{"colab":{"collapsed_sections":["OUKMZp23mSCr","yl5cv19OkMow"],"provenance":[{"file_id":"1SZkrnGtgfuwvfRRP7UBAQ34BwUvmAMUh","timestamp":1705415599157},{"file_id":"1d-GiJwYlC-WdoGu_5vFlltF9tw3xC5NO","timestamp":1676304906256},{"file_id":"1R7bMyDO1RnPXzgWmi4DDWj0Mo-tHPZhw","timestamp":1676299669844}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":0}
